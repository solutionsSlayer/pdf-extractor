#!/usr/bin/env python3
"""
Script de test pour l'extraction LangChain avec Llama 3.1
"""

import json
import logging
from pathlib import Path
from extractor.langchain_extractor import LangChainExtractor

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

def test_single_extraction():
    """Test d'extraction sur un fichier unique"""
    
    # Initialisation de l'extracteur
    extractor = LangChainExtractor(
        model_name="llama3.1:latest",
        base_url="http://localhost:11434"
    )
    
    # Fichier de test
    test_file = "extracted_data/3011360006707/extracted_3011360006707.txt"
    
    if not Path(test_file).exists():
        print(f"‚ùå Fichier de test non trouv√© : {test_file}")
        return
    
    print(f"üîç Test d'extraction sur : {test_file}")
    
    # Extraction
    result = extractor.extract_from_file(test_file)
    
    # Affichage des r√©sultats
    if result.success:
        print("‚úÖ Extraction r√©ussie !")
        print(f"üìä Score de confiance : {result.confidence_score:.2f}")
        
        # Affichage des informations principales
        if result.product_sheet:
            sheet = result.product_sheet
            print("\nüìã Informations extraites :")
            print(f"  ‚Ä¢ Nom du produit : {sheet.product_name}")
            print(f"  ‚Ä¢ D√©nomination l√©gale : {sheet.legal_denomination}")
            print(f"  ‚Ä¢ Code EAN : {sheet.ean_code}")
            print(f"  ‚Ä¢ Nombre d'ingr√©dients : {len(sheet.ingredients) if sheet.ingredients else 0}")
            print(f"  ‚Ä¢ Nombre d'allerg√®nes : {len(sheet.allergens) if sheet.allergens else 0}")
            print(f"  ‚Ä¢ Nombre de valeurs nutritionnelles : {len(sheet.nutritional_values) if sheet.nutritional_values else 0}")
            
            # Sauvegarde du r√©sultat
            output_file = "test_extraction_result.json"
            extractor.save_results_to_json({test_file: result}, output_file)
            print(f"üíæ R√©sultat sauvegard√© dans : {output_file}")
    else:
        print("‚ùå √âchec de l'extraction")
        if result.errors:
            for error in result.errors:
                print(f"  ‚Ä¢ Erreur : {error}")

def test_batch_extraction():
    """Test d'extraction en lot sur tous les fichiers disponibles"""
    
    # Recherche des fichiers d'extraction
    extracted_data_dir = Path("extracted_data")
    
    if not extracted_data_dir.exists():
        print("‚ùå R√©pertoire extracted_data non trouv√©")
        return
    
    # Collecte des fichiers .txt
    txt_files = list(extracted_data_dir.rglob("*.txt"))
    
    if not txt_files:
        print("‚ùå Aucun fichier .txt trouv√© dans extracted_data")
        return
    
    print(f"üîç Test d'extraction en lot sur {len(txt_files)} fichiers")
    
    # Initialisation de l'extracteur
    extractor = LangChainExtractor()
    
    # Extraction en lot
    file_paths = [str(f) for f in txt_files]
    results = extractor.batch_extract(file_paths)
    
    # Analyse des r√©sultats
    successful = sum(1 for r in results.values() if r.success)
    failed = len(results) - successful
    
    print(f"‚úÖ Extractions r√©ussies : {successful}")
    print(f"‚ùå Extractions √©chou√©es : {failed}")
    
    # Calcul du score de confiance moyen
    confidence_scores = [r.confidence_score for r in results.values() if r.success and r.confidence_score]
    if confidence_scores:
        avg_confidence = sum(confidence_scores) / len(confidence_scores)
        print(f"üìä Score de confiance moyen : {avg_confidence:.2f}")
    
    # Sauvegarde des r√©sultats
    output_file = "batch_extraction_results.json"
    extractor.save_results_to_json(results, output_file)
    print(f"üíæ R√©sultats sauvegard√©s dans : {output_file}")

def display_extraction_details(result_file: str):
    """Affiche les d√©tails d'un r√©sultat d'extraction"""
    
    if not Path(result_file).exists():
        print(f"‚ùå Fichier de r√©sultats non trouv√© : {result_file}")
        return
    
    with open(result_file, 'r', encoding='utf-8') as f:
        results = json.load(f)
    
    for file_path, result in results.items():
        print(f"\nüìÑ Fichier : {Path(file_path).name}")
        print(f"   Succ√®s : {'‚úÖ' if result['success'] else '‚ùå'}")
        
        if result['success'] and result['product_sheet']:
            sheet = result['product_sheet']
            print(f"   Score de confiance : {result['confidence_score']:.2f}")
            print(f"   Produit : {sheet.get('product_name', 'N/A')}")
            print(f"   EAN : {sheet.get('ean_code', 'N/A')}")
            
            # Allerg√®nes
            allergens = sheet.get('allergens', [])
            if allergens:
                print(f"   Allerg√®nes pr√©sents :")
                for allergen in allergens:
                    if allergen.get('status') == 'Oui':
                        print(f"     ‚Ä¢ {allergen.get('name')}")
        
        if result.get('errors'):
            print(f"   Erreurs :")
            for error in result['errors']:
                print(f"     ‚Ä¢ {error}")

if __name__ == "__main__":
    print("üöÄ Test de l'extracteur LangChain avec Llama 3.1")
    print("=" * 50)
    
    # V√©rification de la disponibilit√© d'Ollama
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            llama_models = [m for m in models if 'llama3.1' in m.get('name', '').lower()]
            if llama_models:
                print(f"‚úÖ Ollama disponible avec {len(llama_models)} mod√®le(s) Llama 3.1")
            else:
                print("‚ö†Ô∏è  Ollama disponible mais aucun mod√®le Llama 3.1 trouv√©")
                print("   Ex√©cutez : ollama pull llama3.1:latest")
        else:
            print("‚ùå Ollama non accessible")
            print("   V√©rifiez qu'Ollama est d√©marr√© : ollama serve")
    except Exception as e:
        print(f"‚ùå Impossible de v√©rifier Ollama : {e}")
        print("   Assurez-vous qu'Ollama est install√© et d√©marr√©")
    
    print("\n" + "=" * 50)
    
    # Menu interactif
    while True:
        print("\nOptions disponibles :")
        print("1. Test d'extraction sur un fichier unique")
        print("2. Test d'extraction en lot")
        print("3. Afficher les d√©tails d'un r√©sultat")
        print("4. Quitter")
        
        choice = input("\nVotre choix (1-4) : ").strip()
        
        if choice == "1":
            test_single_extraction()
        elif choice == "2":
            test_batch_extraction()
        elif choice == "3":
            result_file = input("Fichier de r√©sultats (ex: test_extraction_result.json) : ").strip()
            display_extraction_details(result_file)
        elif choice == "4":
            print("üëã Au revoir !")
            break
        else:
            print("‚ùå Choix invalide, veuillez r√©essayer") 