# Extraction Structur√©e avec LangChain et Llama 3.1

Cette solution int√®gre LangChain avec votre mod√®le Llama 3.1 local pour transformer vos donn√©es markdown extraites en JSON structur√©.

## üöÄ Fonctionnalit√©s

- **Extraction structur√©e** : Conversion automatique du texte en donn√©es JSON structur√©es
- **Sch√©mas Pydantic** : Validation et typage strict des donn√©es extraites
- **Mod√®le local** : Utilisation de Llama 3.1 via Ollama (pas de d√©pendance cloud)
- **Score de confiance** : √âvaluation automatique de la qualit√© de l'extraction
- **Traitement en lot** : Traitement de multiples fichiers simultan√©ment
- **Configuration flexible** : Personnalisation via variables d'environnement

## üìã Pr√©requis

### 1. Ollama et Llama 3.1

```bash
# Installation d'Ollama (si pas d√©j√† fait)
curl -fsSL https://ollama.ai/install.sh | sh

# T√©l√©chargement du mod√®le Llama 3.1
ollama pull llama3.1:latest

# D√©marrage du serveur Ollama
ollama serve
```

### 2. D√©pendances Python

```bash
pip install -r requirements.txt
```

## üèóÔ∏è Architecture

```
‚îú‚îÄ‚îÄ extractor/
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py              # Sch√©mas Pydantic pour la structure des donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ langchain_extractor.py  # Extracteur LangChain principal
‚îÇ   ‚îî‚îÄ‚îÄ pdf_extractor.py        # Extracteur PDF existant
‚îú‚îÄ‚îÄ config.py                   # Configuration centralis√©e
‚îú‚îÄ‚îÄ main_langchain.py          # Script principal int√©gr√©
‚îú‚îÄ‚îÄ test_langchain_extraction.py # Script de test
‚îî‚îÄ‚îÄ README_LANGCHAIN.md        # Cette documentation
```

## üìä Structure des Donn√©es Extraites

### Sch√©ma Principal : `ProductSheet`

```json
{
  "product_name": "KNORR ROUX BRUN 1KG",
  "legal_denomination": "Roux brun instantan√© d√©shydrat√©",
  "ean_code": "3011360006707",
  "ingredients": ["Farine de BL√â", "graisse de palme", "colorant caramel E150c"],
  "allergens": [
    {
      "name": "C√©r√©ales contenant du gluten",
      "status": "Oui"
    }
  ],
  "nutritional_values": [
    {
      "name": "Valeur √©nerg√©tique",
      "per_100g": "2451kJ - 592kcal",
      "per_100ml_prepared": "245kJ - 59kcal"
    }
  ],
  "product_benefits": [
    "Ma√Ætrisez √† 100% la consistance de vos sauces",
    "Apporte onctuosit√© et brillance √† toutes vos pr√©parations"
  ],
  "vegetarian_suitable": true,
  "vegan_suitable": true,
  "gmo_free": true,
  "extraction_date": "2024-01-15T10:30:00",
  "source_file": "extracted_3011360006707.txt"
}
```

### Types de Donn√©es Support√©s

- **Informations g√©n√©rales** : nom, d√©nomination l√©gale, EAN
- **Composition** : ingr√©dients, additifs
- **Allerg√®nes** : avec statut (Oui/Traces/Non)
- **Nutrition** : valeurs nutritionnelles compl√®tes
- **Caract√©ristiques** : v√©g√©tarien, bio, sans OGM, etc.
- **Logistique** : poids, dimensions, codes EAN
- **Qualit√©** : certifications, normes
- **Contact** : informations fabricant

## üîß Configuration

### Variables d'Environnement

```bash
# Configuration Ollama
export OLLAMA_BASE_URL="http://localhost:11434"
export OLLAMA_MODEL="llama3.1:latest"
export OLLAMA_TEMPERATURE="0.1"
export OLLAMA_NUM_PREDICT="4096"

# Configuration extraction
export MAX_RETRIES="3"
export BATCH_SIZE="5"
export ENABLE_CONFIDENCE_SCORING="true"

# Chemins
export EXTRACTED_DATA_DIR="extracted_data"
export OUTPUT_DIR="structured_output"
export LOGS_DIR="logs"

# Logging
export LOG_LEVEL="INFO"
export LOG_TO_FILE="true"
export LOG_TO_CONSOLE="true"
```

### Fichier `.env` (optionnel)

```env
OLLAMA_MODEL=llama3.1:latest
OLLAMA_TEMPERATURE=0.1
OUTPUT_DIR=structured_output
LOG_LEVEL=INFO
```

## üöÄ Utilisation

### 1. Script Principal Int√©gr√©

```bash
python main_langchain.py
```

Menu interactif avec options :
1. Traiter un fichier PDF unique
2. Traiter tous les PDFs d'un r√©pertoire
3. Traiter les fichiers texte d√©j√† extraits
4. Afficher la configuration
5. Quitter

### 2. Script de Test

```bash
python test_langchain_extraction.py
```

Options de test :
1. Test sur un fichier unique
2. Test en lot
3. Affichage des r√©sultats d√©taill√©s

### 3. Utilisation Programmatique

```python
from extractor.langchain_extractor import LangChainExtractor

# Initialisation
extractor = LangChainExtractor(
    model_name="llama3.1:latest",
    base_url="http://localhost:11434"
)

# Extraction depuis un fichier
result = extractor.extract_from_file("extracted_data/product.txt")

if result.success:
    print(f"Produit : {result.product_sheet.product_name}")
    print(f"Confiance : {result.confidence_score:.2f}")
else:
    print(f"Erreurs : {result.errors}")

# Extraction en lot
file_paths = ["file1.txt", "file2.txt", "file3.txt"]
results = extractor.batch_extract(file_paths)

# Sauvegarde
extractor.save_results_to_json(results, "results.json")
```

## üìà Score de Confiance

Le syst√®me calcule automatiquement un score de confiance (0-1) bas√© sur :

- **Compl√©tude des donn√©es** : nombre de champs remplis
- **Qualit√© des extractions** : coh√©rence des donn√©es
- **Complexit√© des structures** : allerg√®nes, nutrition, logistique

**Interpr√©tation** :
- `0.8-1.0` : Extraction excellente
- `0.6-0.8` : Extraction bonne
- `0.4-0.6` : Extraction moyenne
- `0.0-0.4` : Extraction faible

## üîç Exemple de R√©sultat Complet

```json
{
  "pdf_file": "product_sheet.pdf",
  "success": true,
  "extraction_steps": {
    "pdf_extraction": {
      "success": true,
      "text_file": "extracted_data/product/extracted_product.txt",
      "images_extracted": 5
    },
    "structured_extraction": {
      "success": true,
      "confidence_score": 0.85,
      "errors": null,
      "warnings": null
    }
  },
  "structured_data": {
    "product_name": "KNORR ROUX BRUN 1KG",
    "legal_denomination": "Roux brun instantan√© d√©shydrat√©",
    "ean_code": "3011360006707",
    "ingredients": ["Farine de BL√â", "graisse de palme", "colorant caramel E150c"],
    "allergens": [
      {
        "name": "C√©r√©ales contenant du gluten",
        "status": "Oui"
      }
    ],
    "nutritional_values": [
      {
        "name": "Valeur √©nerg√©tique",
        "per_100g": "2451kJ - 592kcal",
        "per_100ml_prepared": "245kJ - 59kcal"
      }
    ],
    "vegetarian_suitable": true,
    "vegan_suitable": true,
    "organic_product": false,
    "gmo_free": true,
    "extraction_date": "2024-01-15T10:30:00.123456",
    "source_file": "extracted_product.txt"
  }
}
```

## üõ†Ô∏è Personnalisation

### Modification des Sch√©mas

√âditez `extractor/schemas.py` pour ajouter/modifier les champs :

```python
class ProductSheet(BaseModel):
    # Ajout d'un nouveau champ
    custom_field: Optional[str] = Field(None, description="Champ personnalis√©")
    
    # Modification d'un champ existant
    product_name: str = Field(description="Nom du produit (obligatoire)")
```

### Personnalisation des Prompts

Modifiez `extractor/langchain_extractor.py` dans la m√©thode `_get_system_prompt()` :

```python
def _get_system_prompt(self) -> str:
    return f"""Tu es un expert sp√©cialis√© dans MES fiches produits.
    
    INSTRUCTIONS SP√âCIFIQUES :
    - Attention particuli√®re aux allerg√®nes
    - Extraction pr√©cise des valeurs nutritionnelles
    - Respect des formats de dates fran√ßais
    
    {self.parser.get_format_instructions()}"""
```

## üêõ D√©pannage

### Probl√®mes Courants

1. **Ollama non accessible**
   ```bash
   # V√©rifier le statut
   curl http://localhost:11434/api/tags
   
   # Red√©marrer si n√©cessaire
   ollama serve
   ```

2. **Mod√®le non trouv√©**
   ```bash
   # Lister les mod√®les
   ollama list
   
   # T√©l√©charger Llama 3.1
   ollama pull llama3.1:latest
   ```

3. **Erreurs de parsing JSON**
   - R√©duire la temp√©rature : `OLLAMA_TEMPERATURE=0.05`
   - Augmenter `num_predict` : `OLLAMA_NUM_PREDICT=8192`
   - V√©rifier les prompts pour la coh√©rence

4. **Performance lente**
   - R√©duire `batch_size` : `BATCH_SIZE=3`
   - Utiliser un mod√®le plus petit : `llama3.1:8b`
   - Optimiser les prompts

### Logs et Debugging

```bash
# Activer les logs d√©taill√©s
export LOG_LEVEL=DEBUG

# Consulter les logs
tail -f logs/extraction.log

# Tester la connectivit√© Ollama
python -c "
from langchain_ollama import ChatOllama
llm = ChatOllama(model='llama3.1:latest')
print(llm.invoke('Test de connexion'))
"
```

## üìä Performance

### Benchmarks Typiques

- **Fichier simple** (1-2 pages) : 10-30 secondes
- **Fichier complexe** (3-5 pages) : 30-60 secondes
- **Lot de 10 fichiers** : 5-15 minutes

### Optimisations

1. **Mod√®le** : Utiliser `llama3.1:8b` pour plus de rapidit√©
2. **Temp√©rature** : R√©duire √† 0.05 pour plus de coh√©rence
3. **Parall√©lisation** : Traiter plusieurs fichiers simultan√©ment
4. **Cache** : R√©utiliser les extractions d√©j√† r√©alis√©es

## ü§ù Contribution

Pour contribuer √† l'am√©lioration :

1. Testez sur vos propres fiches produits
2. Signalez les erreurs d'extraction
3. Proposez des am√©liorations de sch√©mas
4. Partagez vos optimisations de prompts

## üìù Licence

Ce projet utilise les m√™mes conditions que votre projet principal. 